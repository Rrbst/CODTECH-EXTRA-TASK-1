{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeL_LqBe5gl1",
        "outputId": "c9ffc41a-828e-4b76-9538-dac7431591c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.3.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21692 sha256=e618e3df7aa411b383370db1fa0e047c13ce115feb79a52d1017547eb69f0571\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=18e21f01ed556635b6752245e7e32fd3d367c9efa56697ac3e9144361c68eb57\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n"
          ]
        }
      ],
      "source": [
        "pip install nltk spacy sumy transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4BYFVjb51_7",
        "outputId": "0c2ba9b1-0f2a-467a-adae-16fb5054e8ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "from sumy.utils import get_stop_words\n"
      ],
      "metadata": {
        "id": "Ncg6ExJR6BVo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raSdMMps6IJ6",
        "outputId": "a0ae80d5-0e25-4fcc-84dc-31661db35a48"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define the text (input article)\n",
        "text = \"\"\"Machine learning is a subset of artificial intelligence that allows computers to learn from data and make decisions without being explicitly programmed. Machine learning models can be trained using large datasets, which are processed through algorithms that identify patterns and make predictions. The goal of machine learning is to improve the accuracy of these predictions over time.\"\"\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tokenizing sentences and words\n",
        "sentences = [sent.text for sent in doc.sents]\n",
        "words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "# Display the tokenized sentences and words\n",
        "print(\"Sentences:\", sentences)\n",
        "print(\"Words:\", words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NQJmQBJ7BdE",
        "outputId": "fa28c480-a9ca-441a-baaa-3816cb42ae4e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Machine learning is a subset of artificial intelligence that allows computers to learn from data and make decisions without being explicitly programmed.', 'Machine learning models can be trained using large datasets, which are processed through algorithms that identify patterns and make predictions.', 'The goal of machine learning is to improve the accuracy of these predictions over time.']\n",
            "Words: ['Machine', 'learning', 'subset', 'artificial', 'intelligence', 'allows', 'computers', 'learn', 'data', 'decisions', 'explicitly', 'programmed', 'Machine', 'learning', 'models', 'trained', 'large', 'datasets', 'processed', 'algorithms', 'identify', 'patterns', 'predictions', 'goal', 'machine', 'learning', 'improve', 'accuracy', 'predictions', 'time']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi9HV40z7-LK",
        "outputId": "adfe222a-35be-46c0-b302-5424e62969ae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "from sumy.utils import get_stop_words\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sumy.nlp.tokenizers import Tokenizer # Import the Tokenizer class\n",
        "\n",
        "# Download 'punkt_tab' which contains the necessary data for sentence tokenization\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Define the text (input article)\n",
        "text = \"\"\"Machine learning is a subset of artificial intelligence that allows computers to learn from data and make decisions without being explicitly programmed. Machine learning models can be trained using large datasets, which are processed through algorithms that identify patterns and make predictions. The goal of machine learning is to improve the accuracy of these predictions over time.\"\"\"\n",
        "\n",
        "# Tokenize sentences using NLTK\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Join sentences back into a string\n",
        "text = \" \".join(sentences)\n",
        "\n",
        "# Create a parser with the tokenized text\n",
        "parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "\n",
        "# Create the summarizer\n",
        "summarizer = LsaSummarizer()\n",
        "summarizer.stop_words = get_stop_words(\"english\")\n",
        "\n",
        "# Generate the summary (3 sentences)\n",
        "summary = summarizer(parser.document, 3)\n",
        "\n",
        "# Output the summary\n",
        "for sentence in summary:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8TokuU39cCM",
        "outputId": "c80dc4f2-a8e2-4905-8743-cd3cd1a262e0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning is a subset of artificial intelligence that allows computers to learn from data and make decisions without being explicitly programmed.\n",
            "Machine learning models can be trained using large datasets, which are processed through algorithms that identify patterns and make predictions.\n",
            "The goal of machine learning is to improve the accuracy of these predictions over time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Define the input text (your article)\n",
        "text = \"\"\"\n",
        "Machine learning is a subset of artificial intelligence that allows computers to learn from data and make decisions without being explicitly programmed. Machine learning models can be trained using large datasets, which are processed through algorithms that identify patterns and make predictions. The goal of machine learning is to improve the accuracy of these predictions over time.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the summarization pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Generate the summary\n",
        "summary = summarizer(text, max_length=150, min_length=30, do_sample=False)\n",
        "\n",
        "# Output the summary\n",
        "print(summary[0]['summary_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq0Ak5tF-ZYa",
        "outputId": "2bdc4eb2-f86f-4615-e96b-96e465faa8b5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n",
            "Your max_length is set to 150, but your input_length is only 66. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Machine learning is a subset of artificial intelligence that allows computers to learn from data and make decisions without being explicitly programmed . Machine learning models can be trained using large datasets, which are processed through algorithms that identify patterns and make predictions .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = input(\"Enter the article text: \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94G0JthQ-xXO",
        "outputId": "48569397-1f48-44aa-f815-005fff231746"
      },
      "execution_count": 26,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the article text: machine learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Summary:\")\n",
        "for sentence in summary:\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJjRHIGj---o",
        "outputId": "a75b6c75-6fc1-40e5-e8a4-8ee6865c23e9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "{'summary_text': ' Machine learning is a subset of artificial intelligence that allows computers to learn from data and make decisions without being explicitly programmed . Machine learning models can be trained using large datasets, which are processed through algorithms that identify patterns and make predictions .'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from transformers import pipeline\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "from sumy.utils import get_stop_words\n",
        "\n",
        "# Step 1: Load SpaCy NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Step 2: Define function for Extractive Summarization\n",
        "def extractive_summary(text, num_sentences=3):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LsaSummarizer()\n",
        "    summarizer.stop_words = get_stop_words(\"english\")\n",
        "    summary = summarizer(parser.document, num_sentences)\n",
        "    return [str(sentence) for sentence in summary]\n",
        "\n",
        "# Step 3: Define function for Abstractive Summarization using BART/T5\n",
        "def abstractive_summary(text):\n",
        "    summarizer = pipeline(\"summarization\")\n",
        "    summary = summarizer(text, max_length=150, min_length=30, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "# Step 4: Input Text\n",
        "text = input(\"Enter the article text: \")\n",
        "\n",
        "# Step 5: Choose Summarization Method\n",
        "method = input(\"Choose summarization method (extractive/abstractive): \")\n",
        "\n",
        "if method == \"extractive\":\n",
        "    summary = extractive_summary(text)\n",
        "    print(\"\\nExtractive Summary:\")\n",
        "    for sentence in summary:\n",
        "        print(sentence)\n",
        "elif method == \"abstractive\":\n",
        "    summary = abstractive_summary(text)\n",
        "    print(\"\\nAbstractive Summary:\")\n",
        "    print(summary)\n",
        "else:\n",
        "    print(\"Invalid method selected!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR7UXQKy_FF5",
        "outputId": "63f9abeb-fcf7-4670-f578-fc6ef11bd3ef"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the article text: The goal of machine learning is to improve the accuracy of these predictions over time.\n",
            "Choose summarization method (extractive/abstractive): extractive\n",
            "\n",
            "Extractive Summary:\n",
            "The goal of machine learning is to improve the accuracy of these predictions over time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Define a function to split the input text into smaller chunks if it's too large\n",
        "def split_text(text, max_length=1024):\n",
        "    # Split the text into chunks of max_length\n",
        "    text_chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
        "    return text_chunks\n",
        "\n",
        "# Initialize the Hugging Face summarization pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Define the input text (the article you're summarizing)\n",
        "text = \"\"\"\n",
        "Machine learning is a subset of artificial intelligence that allows computers to learn from data and make decisions without being explicitly programmed. Machine learning models can be trained using large datasets, which are processed through algorithms that identify patterns and make predictions. The goal of machine learning is to improve the accuracy of these predictions over time. It is used in many different applications, including natural language processing, image recognition, and autonomous vehicles. One of the most popular approaches in machine learning is deep learning, which involves using neural networks to model complex patterns in data. In this approach, large amounts of labeled data are used to train the model, which can then make predictions on new, unseen data. Deep learning models have achieved remarkable success in tasks such as image classification and language translation, and they continue to be a key area of research in the field of artificial intelligence.\n",
        "\"\"\"\n",
        "\n",
        "# Split the text into smaller chunks if it exceeds the model's input length\n",
        "chunks = split_text(text, max_length=1024)\n",
        "\n",
        "# Summarize each chunk\n",
        "summaries = []\n",
        "for chunk in chunks:\n",
        "    summary = summarizer(chunk, max_length=150, min_length=30, do_sample=False)\n",
        "    summaries.append(summary[0]['summary_text'])\n",
        "\n",
        "# Concatenate the summaries to form a larger summary\n",
        "final_summary = \" \".join(summaries)\n",
        "\n",
        "# Print the final summary\n",
        "print(final_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj16_PCf_o75",
        "outputId": "a30cb2f6-d805-4ed5-e6b1-4e3594e01517"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Machine learning is a subset of artificial intelligence that allows computers to learn from data and make decisions without being explicitly programmed . Machine learning models can be trained using large datasets, which are processed through algorithms that identify patterns and make predictions .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from transformers import pipeline\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "from sumy.utils import get_stop_words\n",
        "\n",
        "# Step 1: Load SpaCy NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Step 2: Define function for Extractive Summarization\n",
        "def extractive_summary(text, num_sentences=50):  # Increased number of sentences for longer summary\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LsaSummarizer()\n",
        "    summarizer.stop_words = get_stop_words(\"english\")\n",
        "    summary = summarizer(parser.document, num_sentences)\n",
        "    return [str(sentence) for sentence in summary]\n",
        "\n",
        "# Step 3: Define function for Abstractive Summarization using BART/T5\n",
        "def abstractive_summary(text):\n",
        "    summarizer = pipeline(\"summarization\")\n",
        "    summary = summarizer(text, max_length=500, min_length=100, do_sample=False)  # Increase max_length for longer summary\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "# Step 4: Input Text\n",
        "text = input(\"Enter the article text: \")\n",
        "\n",
        "# Step 5: Choose Summarization Method\n",
        "method = input(\"Choose summarization method (extractive/abstractive): \")\n",
        "\n",
        "if method == \"extractive\":\n",
        "    summary = extractive_summary(text, num_sentences=50)  # Increase number of sentences for a longer summary\n",
        "    print(\"\\nExtractive Summary:\")\n",
        "    for sentence in summary:\n",
        "        print(sentence)\n",
        "elif method == \"abstractive\":\n",
        "    summary = abstractive_summary(text)\n",
        "    print(\"\\nAbstractive Summary:\")\n",
        "    print(summary)\n",
        "else:\n",
        "    print(\"Invalid method selected!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYbJoMjiAEl-",
        "outputId": "c99dd945-43ce-4b5f-cc58-9a86eeebac4e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the article text: Machine learning is a pivotal aspect of artificial intelligence that focuses on the development of algorithms and statistical models designed to enable systems to learn from and make decisions based on data. Unlike traditional programming, where explicit instructions are provided to perform tasks, machine learning allows computers to identify patterns, derive insights, and improve over time without human intervention. Machine learning techniques are broadly classified into supervised learning, unsupervised learning, and reinforcement learning, each serving specific purposes depending on the structure and nature of the data. Supervised learning is employed when labeled datasets are available to train models for tasks such as classification and regression, while unsupervised learning is used when the data is unlabelled, aiming to uncover hidden patterns or groupings, such as in clustering and dimensionality reduction. Reinforcement learning, on the other hand, is used for decision-making processes, where an agent learns to take actions by interacting with an environment, receiving feedback in the form of rewards or penalties. The applications of machine learning span a wide range of industries, from healthcare, where it aids in diagnosing diseases and predicting treatment outcomes, to finance, where it supports fraud detection and risk analysis, and even to entertainment, powering recommendation engines on platforms like Netflix and YouTube. As more data is collected and processed, machine learning models continue to evolve, becoming increasingly accurate and effective, with a growing role in automating tasks, optimizing processes, and enhancing user experiences across various sectors.\n",
            "Choose summarization method (extractive/abstractive): extractive\n",
            "\n",
            "Extractive Summary:\n",
            "Machine learning is a pivotal aspect of artificial intelligence that focuses on the development of algorithms and statistical models designed to enable systems to learn from and make decisions based on data.\n",
            "Unlike traditional programming, where explicit instructions are provided to perform tasks, machine learning allows computers to identify patterns, derive insights, and improve over time without human intervention.\n",
            "Machine learning techniques are broadly classified into supervised learning, unsupervised learning, and reinforcement learning, each serving specific purposes depending on the structure and nature of the data.\n",
            "Supervised learning is employed when labeled datasets are available to train models for tasks such as classification and regression, while unsupervised learning is used when the data is unlabelled, aiming to uncover hidden patterns or groupings, such as in clustering and dimensionality reduction.\n",
            "Reinforcement learning, on the other hand, is used for decision-making processes, where an agent learns to take actions by interacting with an environment, receiving feedback in the form of rewards or penalties.\n",
            "The applications of machine learning span a wide range of industries, from healthcare, where it aids in diagnosing diseases and predicting treatment outcomes, to finance, where it supports fraud detection and risk analysis, and even to entertainment, powering recommendation engines on platforms like Netflix and YouTube.\n",
            "As more data is collected and processed, machine learning models continue to evolve, becoming increasingly accurate and effective, with a growing role in automating tasks, optimizing processes, and enhancing user experiences across various sectors.\n"
          ]
        }
      ]
    }
  ]
}